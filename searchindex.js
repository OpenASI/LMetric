Search.setIndex({"docnames": ["index", "lmetric", "lmetric.attention", "lmetric.floating_ops", "lmetric.llamacpp", "lmetric.llm_prune", "lmetric.llm_quant", "lmetric.utils", "modules"], "filenames": ["index.rst", "lmetric.rst", "lmetric.attention.rst", "lmetric.floating_ops.rst", "lmetric.llamacpp.rst", "lmetric.llm_prune.rst", "lmetric.llm_quant.rst", "lmetric.utils.rst", "modules.rst"], "titles": ["Welcome to lmetric\u2019s documentation!", "lmetric package", "lmetric.attention package", "lmetric.floating_ops package", "lmetric.llamacpp package", "lmetric.llm_prune package", "lmetric.llm_quant package", "lmetric.utils package", "lmetric"], "terms": {"packag": [0, 8], "index": 0, "modul": [0, 8], "search": 0, "page": 0, "attent": [1, 8], "submodul": [1, 8], "lmetric_attent": [1, 8], "attention_pytorch": [1, 2], "floating_op": [1, 8], "lmetric_matmul": [1, 8], "lmetric_matmul_fp16": [1, 3], "matmul": [1, 3], "llamacpp": [1, 8], "execute_llamacpp": [1, 8], "lmetric_llamacpp": [1, 8], "util": [1, 8], "convert_to_json": [1, 4], "get_arg_pars": [1, 4], "preprocess_arg": [1, 4], "llm_prune": [1, 8], "lmetric_prune_matmul": [1, 8], "get_coo": [1, 5], "get_pt_spars": [1, 5], "lmetric_prune_matmul_lx3hxh": [1, 5], "llm_quant": [1, 8], "custom_autotun": [1, 8], "autotun": [1, 6], "prune_config": [1, 6], "run": [1, 6], "warmup": [1, 6], "matmul248_kernel_config_prun": [1, 6], "gptq_quant_linear": [1, 8], "quantlinear": [1, 6], "forward": [1, 2, 6], "pack": [1, 6], "quantlinearfunct": [1, 6], "backward": [1, 2, 6], "autotune_warmup_linear": [1, 6], "make_quant_linear": [1, 6], "matmul248": [1, 6], "transpose_matmul248": [1, 6], "lmetric_quant_matmul": [1, 8], "get_bnb_weight_st": [1, 6], "lmetric_quant_matmul_lx3hxh": [1, 6], "hf_download": [1, 8], "download_hf_repo": [1, 7], "qkv": 2, "dropout_p": 2, "0": [2, 5, 6], "causal": 2, "true": [2, 6], "sourc": [2, 3, 4, 5, 6, 7], "paramet": [2, 3, 5, 6, 7], "batch_siz": 2, "seqlen": [2, 5, 6], "3": 2, "nhead": 2, "head_dim": 2, "float": [2, 5, 6], "output": [2, 6], "int": [2, 5, 6], "32": 2, "128": [2, 5, 6], "num_head": 2, "64": 2, "96": 2, "160": 2, "dtype": [2, 5, 6], "torch": [2, 3, 5, 6], "float16": [2, 5, 6], "metric": [2, 3, 5, 6], "tflop": [2, 3, 5, 6], "kernel": [2, 3, 5, 6], "triton_flash": 2, "flash2": 2, "xformer": 2, "direct": 2, "fwd": 2, "perform": [2, 3, 6], "benchmark": [2, 6], "differ": [2, 6], "list": [2, 3, 5, 6, 7], "batch": 2, "size": [2, 3, 5, 6], "input": [2, 5, 6], "data": [2, 3, 5, 6], "default": [2, 3, 5, 6, 7], "i": [2, 3, 6], "sequenc": [2, 5, 6], "length": [2, 5, 6], "number": [2, 6], "head": 2, "The": [2, 3, 6], "dimens": [2, 6], "onli": 2, "support": 2, "256": [2, 3], "type": [2, 3, 5, 6, 7], "tensor": [2, 5, 6], "str": [2, 3, 5, 6, 7], "measur": [2, 3], "avail": 2, "option": [2, 3, 5, 6, 7], "ar": [2, 3, 6], "tbp": [2, 3, 5, 6], "m": [2, 3, 5, 6], "function": [2, 6], "us": [2, 3, 6, 7], "comput": [2, 5, 6], "bwd": 2, "fwd_bwd": 2, "return": [2, 3, 5, 6, 7], "none": [2, 3, 6, 7], "896": 3, "1536": 3, "2176": 3, "2816": 3, "3456": 3, "4096": [3, 5, 6], "triton": [3, 6], "matrix": [3, 5, 6], "multipl": [3, 5, 6], "librari": 3, "fp16": [3, 6], "matric": 3, "valid": 3, "we": 3, "test": 3, "b": 3, "activ": 3, "arrai": 4, "arg": [4, 6], "x": [5, 6], "threshold": [5, 6], "coo": 5, "spars": 5, "represent": 5, "base": [5, 6], "given": [5, 6], "valu": [5, 6], "coosparsetensor": 5, "pytorch": 5, "csr": 5, "csc": 5, "bsr": 5, "bsc": 5, "hidden_s": [5, 6], "2048": [5, 6], "bnb_spars": 5, "ratio": 5, "5": 5, "prune": [5, 6], "oper": [5, 6], "hidden": [5, 6], "layer": 5, "torch_spars": 5, "mostli": 6, "same": 6, "few": 6, "chang": 6, "like": 6, "40": 6, "instead": 6, "100": 6, "class": 6, "fn": 6, "arg_nam": 6, "config": 6, "kei": 6, "reset_to_zero": 6, "prune_configs_bi": 6, "dict": 6, "nearest_power_of_two": 6, "bool": [6, 7], "fals": [6, 7], "iter": 6, "debug": 6, "kernelinterfac": 6, "kwarg": 6, "decor": 6, "auto": 6, "tune": 6, "jit": 6, "d": 6, "highlight": 6, "python": 6, "code": 6, "block": 6, "meta": 6, "block_siz": 6, "num_warp": 6, "4": 6, "1024": 6, "8": 6, "x_size": 6, "two": 6, "abov": 6, "evalu": 6, "anytim": 6, "def": 6, "x_ptr": 6, "note": 6, "when": [6, 7], "all": 6, "configur": 6, "time": 6, "thi": 6, "mean": 6, "whatev": 6, "updat": 6, "To": 6, "avoid": 6, "undesir": 6, "behavior": 6, "you": 6, "can": 6, "argument": 6, "which": 6, "reset": 6, "provid": 6, "zero": 6, "befor": 6, "ani": 6, "object": 6, "name": [6, 7], "whose": 6, "trigger": 6, "field": 6, "perf_model": 6, "model": 6, "predic": 6, "top_k": 6, "bench": 6, "early_config_prun": 6, "do": 6, "earli": 6, "eg": 6, "num_stag": 6, "It": 6, "take": 6, "its": 6, "narg": 6, "main": 6, "purpos": 6, "shrink": 6, "block_size_": 6, "correspond": 6, "smaller": 6, "bit": 6, "groupsiz": 6, "infeatur": 6, "outfeatur": 6, "bia": 6, "defin": 6, "everi": 6, "call": 6, "should": 6, "overridden": 6, "subclass": 6, "although": 6, "recip": 6, "pass": 6, "need": 6, "within": 6, "one": 6, "instanc": 6, "afterward": 6, "sinc": 6, "former": 6, "care": 6, "regist": 6, "hook": 6, "while": 6, "latter": 6, "silent": 6, "ignor": 6, "them": 6, "linear": 6, "scale": 6, "g_idx": 6, "static": 6, "ctx": 6, "grad_output": 6, "formula": 6, "differenti": 6, "mode": 6, "automat": 6, "alia": 6, "vjp": 6, "must": 6, "accept": 6, "context": 6, "first": 6, "follow": 6, "mani": 6, "non": 6, "were": 6, "each": 6, "gradient": 6, "w": 6, "r": 6, "t": 6, "If": 6, "an": 6, "requir": 6, "grad": 6, "just": 6, "retriev": 6, "save": [6, 7], "dure": 6, "also": 6, "ha": 6, "attribut": 6, "needs_input_grad": 6, "tupl": 6, "boolean": 6, "repres": 6, "whether": [6, 7], "e": 6, "g": 6, "have": 6, "qweight": 6, "qzero": 6, "maxq": 6, "There": 6, "wai": 6, "usag": 6, "1": 6, "combin": 6, "staticmethod": 6, "other": 6, "see": 6, "more": 6, "detail": 6, "2": 6, "separ": 6, "setup_context": 6, "longer": 6, "overrid": 6, "autograd": 6, "handl": 6, "set": 6, "up": 6, "extend": 6, "store": 6, "arbitrari": 6, "directli": 6, "though": 6, "current": 6, "enforc": 6, "compat": 6, "either": 6, "save_for_backward": 6, "thei": 6, "intend": 6, "equival": 6, "save_for_forward": 6, "jvp": 6, "transpos": 6, "pre": 6, "quantiz": 6, "weight": 6, "6": 6, "has_fp16_weight": 6, "memory_efficient_backward": 6, "construct": 6, "state": 6, "bnb_weight": 6, "dictionari": 6, "flag": 6, "indic": 6, "format": 6, "memori": 6, "effici": 6, "enabl": 6, "A": 6, "contain": 6, "6144": 6, "8192": 6, "10240": 6, "12288": 6, "14336": 6, "gptq": 6, "bnb_infer": 6, "rang": 6, "group": 6, "repo_id": 7, "repo_typ": 7, "filenam": 7, "white_pattern": 7, "black_pattern": 7, "save_dir": 7, "save_with_symlink": 7, "revis": 7, "download": 7, "repositori": 7, "from": 7, "hug": 7, "face": 7, "hub": 7, "creat": 7, "snapshot": 7, "identifi": 7, "file": 7, "pattern": 7, "includ": 7, "exclud": 7, "directori": 7, "symlink": 7, "local": 7, "rais": 7, "someexcept": 7, "descript": 7, "except": 7, "exampl": 7, "nousresearch": 7, "llama2": 7, "7b": 7, "hf": 7, "subpackag": 8, "content": 8}, "objects": {"": [[1, 0, 0, "-", "lmetric"]], "lmetric": [[2, 0, 0, "-", "attention"], [3, 0, 0, "-", "floating_ops"], [4, 0, 0, "-", "llamacpp"], [5, 0, 0, "-", "llm_prune"], [6, 0, 0, "-", "llm_quant"], [7, 0, 0, "-", "utils"]], "lmetric.attention": [[2, 0, 0, "-", "lmetric_attention"]], "lmetric.attention.lmetric_attention": [[2, 1, 1, "", "attention_pytorch"], [2, 1, 1, "", "lmetric_attention"]], "lmetric.floating_ops": [[3, 0, 0, "-", "lmetric_matmul"]], "lmetric.floating_ops.lmetric_matmul": [[3, 1, 1, "", "lmetric_matmul_fp16"], [3, 1, 1, "", "matmul"]], "lmetric.llamacpp": [[4, 0, 0, "-", "utils"]], "lmetric.llamacpp.utils": [[4, 1, 1, "", "convert_to_json"], [4, 1, 1, "", "get_arg_parser"], [4, 1, 1, "", "preprocess_args"]], "lmetric.llm_prune": [[5, 0, 0, "-", "lmetric_prune_matmul"]], "lmetric.llm_prune.lmetric_prune_matmul": [[5, 1, 1, "", "get_coo"], [5, 1, 1, "", "get_pt_sparse"], [5, 1, 1, "", "lmetric_prune_matmul_Lx3HxH"]], "lmetric.llm_quant": [[6, 0, 0, "-", "custom_autotune"], [6, 0, 0, "-", "gptq_quant_linear"], [6, 0, 0, "-", "lmetric_quant_matmul"]], "lmetric.llm_quant.custom_autotune": [[6, 2, 1, "", "Autotuner"], [6, 1, 1, "", "autotune"], [6, 1, 1, "", "matmul248_kernel_config_pruner"]], "lmetric.llm_quant.custom_autotune.Autotuner": [[6, 3, 1, "", "prune_configs"], [6, 3, 1, "", "run"], [6, 3, 1, "", "warmup"]], "lmetric.llm_quant.gptq_quant_linear": [[6, 2, 1, "", "QuantLinear"], [6, 2, 1, "", "QuantLinearFunction"], [6, 1, 1, "", "autotune_warmup_linear"], [6, 1, 1, "", "make_quant_linear"], [6, 1, 1, "", "matmul248"], [6, 1, 1, "", "transpose_matmul248"]], "lmetric.llm_quant.gptq_quant_linear.QuantLinear": [[6, 3, 1, "", "forward"], [6, 3, 1, "", "pack"]], "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction": [[6, 3, 1, "", "backward"], [6, 3, 1, "", "forward"]], "lmetric.llm_quant.lmetric_quant_matmul": [[6, 1, 1, "", "get_bnb_weight_state"], [6, 1, 1, "", "lmetric_quant_matmul_Lx3HxH"]], "lmetric.utils": [[7, 0, 0, "-", "hf_download"]], "lmetric.utils.hf_download": [[7, 1, 1, "", "download_hf_repo"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"]}, "titleterms": {"welcom": 0, "lmetric": [0, 1, 2, 3, 4, 5, 6, 7, 8], "": 0, "document": 0, "content": [0, 1, 2, 3, 4, 5, 6, 7], "indic": 0, "tabl": 0, "packag": [1, 2, 3, 4, 5, 6, 7], "subpackag": 1, "modul": [1, 2, 3, 4, 5, 6, 7], "attent": 2, "submodul": [2, 3, 4, 5, 6, 7], "lmetric_attent": 2, "floating_op": 3, "lmetric_matmul": 3, "llamacpp": 4, "execute_llamacpp": 4, "lmetric_llamacpp": 4, "util": [4, 7], "llm_prune": 5, "lmetric_prune_matmul": 5, "llm_quant": 6, "custom_autotun": 6, "gptq_quant_linear": 6, "lmetric_quant_matmul": 6, "hf_download": 7}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1, "sphinx": 57}, "alltitles": {"Welcome to lmetric\u2019s documentation!": [[0, "welcome-to-lmetric-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "lmetric package": [[1, "lmetric-package"]], "Subpackages": [[1, "subpackages"]], "Module contents": [[1, "module-lmetric"], [2, "module-lmetric.attention"], [3, "module-lmetric.floating_ops"], [4, "module-lmetric.llamacpp"], [5, "module-lmetric.llm_prune"], [6, "module-lmetric.llm_quant"], [7, "module-lmetric.utils"]], "lmetric.attention package": [[2, "lmetric-attention-package"]], "Submodules": [[2, "submodules"], [3, "submodules"], [4, "submodules"], [5, "submodules"], [6, "submodules"], [7, "submodules"]], "lmetric.attention.lmetric_attention module": [[2, "module-lmetric.attention.lmetric_attention"]], "lmetric.floating_ops package": [[3, "lmetric-floating-ops-package"]], "lmetric.floating_ops.lmetric_matmul module": [[3, "module-lmetric.floating_ops.lmetric_matmul"]], "lmetric.llamacpp package": [[4, "lmetric-llamacpp-package"]], "lmetric.llamacpp.execute_llamacpp module": [[4, "lmetric-llamacpp-execute-llamacpp-module"]], "lmetric.llamacpp.lmetric_llamacpp module": [[4, "lmetric-llamacpp-lmetric-llamacpp-module"]], "lmetric.llamacpp.utils module": [[4, "module-lmetric.llamacpp.utils"]], "lmetric.llm_prune package": [[5, "lmetric-llm-prune-package"]], "lmetric.llm_prune.lmetric_prune_matmul module": [[5, "module-lmetric.llm_prune.lmetric_prune_matmul"]], "lmetric.llm_quant package": [[6, "lmetric-llm-quant-package"]], "lmetric.llm_quant.custom_autotune module": [[6, "module-lmetric.llm_quant.custom_autotune"]], "lmetric.llm_quant.gptq_quant_linear module": [[6, "module-lmetric.llm_quant.gptq_quant_linear"]], "lmetric.llm_quant.lmetric_quant_matmul module": [[6, "module-lmetric.llm_quant.lmetric_quant_matmul"]], "lmetric.utils package": [[7, "lmetric-utils-package"]], "lmetric.utils.hf_download module": [[7, "module-lmetric.utils.hf_download"]], "lmetric": [[8, "lmetric"]]}, "indexentries": {"lmetric": [[1, "module-lmetric"]], "module": [[1, "module-lmetric"], [2, "module-lmetric.attention"], [2, "module-lmetric.attention.lmetric_attention"], [3, "module-lmetric.floating_ops"], [3, "module-lmetric.floating_ops.lmetric_matmul"], [4, "module-lmetric.llamacpp"], [4, "module-lmetric.llamacpp.utils"], [5, "module-lmetric.llm_prune"], [5, "module-lmetric.llm_prune.lmetric_prune_matmul"], [6, "module-lmetric.llm_quant"], [6, "module-lmetric.llm_quant.custom_autotune"], [6, "module-lmetric.llm_quant.gptq_quant_linear"], [6, "module-lmetric.llm_quant.lmetric_quant_matmul"], [7, "module-lmetric.utils"], [7, "module-lmetric.utils.hf_download"]], "attention_pytorch() (in module lmetric.attention.lmetric_attention)": [[2, "lmetric.attention.lmetric_attention.attention_pytorch"]], "lmetric.attention": [[2, "module-lmetric.attention"]], "lmetric.attention.lmetric_attention": [[2, "module-lmetric.attention.lmetric_attention"]], "lmetric_attention() (in module lmetric.attention.lmetric_attention)": [[2, "lmetric.attention.lmetric_attention.lmetric_attention"]], "lmetric.floating_ops": [[3, "module-lmetric.floating_ops"]], "lmetric.floating_ops.lmetric_matmul": [[3, "module-lmetric.floating_ops.lmetric_matmul"]], "lmetric_matmul_fp16() (in module lmetric.floating_ops.lmetric_matmul)": [[3, "lmetric.floating_ops.lmetric_matmul.lmetric_matmul_fp16"]], "matmul() (in module lmetric.floating_ops.lmetric_matmul)": [[3, "lmetric.floating_ops.lmetric_matmul.matmul"]], "convert_to_json() (in module lmetric.llamacpp.utils)": [[4, "lmetric.llamacpp.utils.convert_to_json"]], "get_arg_parser() (in module lmetric.llamacpp.utils)": [[4, "lmetric.llamacpp.utils.get_arg_parser"]], "lmetric.llamacpp": [[4, "module-lmetric.llamacpp"]], "lmetric.llamacpp.utils": [[4, "module-lmetric.llamacpp.utils"]], "preprocess_args() (in module lmetric.llamacpp.utils)": [[4, "lmetric.llamacpp.utils.preprocess_args"]], "get_coo() (in module lmetric.llm_prune.lmetric_prune_matmul)": [[5, "lmetric.llm_prune.lmetric_prune_matmul.get_coo"]], "get_pt_sparse() (in module lmetric.llm_prune.lmetric_prune_matmul)": [[5, "lmetric.llm_prune.lmetric_prune_matmul.get_pt_sparse"]], "lmetric.llm_prune": [[5, "module-lmetric.llm_prune"]], "lmetric.llm_prune.lmetric_prune_matmul": [[5, "module-lmetric.llm_prune.lmetric_prune_matmul"]], "lmetric_prune_matmul_lx3hxh() (in module lmetric.llm_prune.lmetric_prune_matmul)": [[5, "lmetric.llm_prune.lmetric_prune_matmul.lmetric_prune_matmul_Lx3HxH"]], "autotuner (class in lmetric.llm_quant.custom_autotune)": [[6, "lmetric.llm_quant.custom_autotune.Autotuner"]], "quantlinear (class in lmetric.llm_quant.gptq_quant_linear)": [[6, "lmetric.llm_quant.gptq_quant_linear.QuantLinear"]], "quantlinearfunction (class in lmetric.llm_quant.gptq_quant_linear)": [[6, "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction"]], "autotune() (in module lmetric.llm_quant.custom_autotune)": [[6, "lmetric.llm_quant.custom_autotune.autotune"]], "autotune_warmup_linear() (in module lmetric.llm_quant.gptq_quant_linear)": [[6, "lmetric.llm_quant.gptq_quant_linear.autotune_warmup_linear"]], "backward() (lmetric.llm_quant.gptq_quant_linear.quantlinearfunction static method)": [[6, "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.backward"]], "forward() (lmetric.llm_quant.gptq_quant_linear.quantlinear method)": [[6, "lmetric.llm_quant.gptq_quant_linear.QuantLinear.forward"]], "forward() (lmetric.llm_quant.gptq_quant_linear.quantlinearfunction static method)": [[6, "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward"]], "get_bnb_weight_state() (in module lmetric.llm_quant.lmetric_quant_matmul)": [[6, "lmetric.llm_quant.lmetric_quant_matmul.get_bnb_weight_state"]], "lmetric.llm_quant": [[6, "module-lmetric.llm_quant"]], "lmetric.llm_quant.custom_autotune": [[6, "module-lmetric.llm_quant.custom_autotune"]], "lmetric.llm_quant.gptq_quant_linear": [[6, "module-lmetric.llm_quant.gptq_quant_linear"]], "lmetric.llm_quant.lmetric_quant_matmul": [[6, "module-lmetric.llm_quant.lmetric_quant_matmul"]], "lmetric_quant_matmul_lx3hxh() (in module lmetric.llm_quant.lmetric_quant_matmul)": [[6, "lmetric.llm_quant.lmetric_quant_matmul.lmetric_quant_matmul_Lx3HxH"]], "make_quant_linear() (in module lmetric.llm_quant.gptq_quant_linear)": [[6, "lmetric.llm_quant.gptq_quant_linear.make_quant_linear"]], "matmul248() (in module lmetric.llm_quant.gptq_quant_linear)": [[6, "lmetric.llm_quant.gptq_quant_linear.matmul248"]], "matmul248_kernel_config_pruner() (in module lmetric.llm_quant.custom_autotune)": [[6, "lmetric.llm_quant.custom_autotune.matmul248_kernel_config_pruner"]], "pack() (lmetric.llm_quant.gptq_quant_linear.quantlinear method)": [[6, "lmetric.llm_quant.gptq_quant_linear.QuantLinear.pack"]], "prune_configs() (lmetric.llm_quant.custom_autotune.autotuner method)": [[6, "lmetric.llm_quant.custom_autotune.Autotuner.prune_configs"]], "run() (lmetric.llm_quant.custom_autotune.autotuner method)": [[6, "lmetric.llm_quant.custom_autotune.Autotuner.run"]], "transpose_matmul248() (in module lmetric.llm_quant.gptq_quant_linear)": [[6, "lmetric.llm_quant.gptq_quant_linear.transpose_matmul248"]], "warmup() (lmetric.llm_quant.custom_autotune.autotuner method)": [[6, "lmetric.llm_quant.custom_autotune.Autotuner.warmup"]], "download_hf_repo() (in module lmetric.utils.hf_download)": [[7, "lmetric.utils.hf_download.download_hf_repo"]], "lmetric.utils": [[7, "module-lmetric.utils"]], "lmetric.utils.hf_download": [[7, "module-lmetric.utils.hf_download"]]}})