Search.setIndex({"docnames": ["index", "lmetric", "lmetric.attention", "lmetric.floating_ops", "lmetric.llm_prune", "lmetric.llm_quant", "lmetric.utils", "modules"], "filenames": ["index.rst", "lmetric.rst", "lmetric.attention.rst", "lmetric.floating_ops.rst", "lmetric.llm_prune.rst", "lmetric.llm_quant.rst", "lmetric.utils.rst", "modules.rst"], "titles": ["Welcome to lmetric\u2019s documentation!", "lmetric package", "lmetric.attention package", "lmetric.floating_ops package", "lmetric.llm_prune package", "lmetric.llm_quant package", "lmetric.utils package", "lmetric"], "terms": {"packag": [0, 7], "index": 0, "modul": [0, 7], "search": 0, "page": 0, "attent": [1, 7], "submodul": [1, 7], "lmetric_attent": [1, 7], "attention_pytorch": [1, 2], "floating_op": [1, 7], "lmetric_matmul": [1, 7], "lmetric_matmul_fp16": [1, 3], "matmul": [1, 3], "llm_prune": [1, 7], "lmetric_prune_matmul": [1, 7], "get_coo": [1, 4], "get_pt_spars": [1, 4], "lmetric_prune_matmul_lx3hxh": [1, 4], "llm_quant": [1, 7], "custom_autotun": [1, 7], "autotun": [1, 5], "prune_config": [1, 5], "run": [1, 5], "warmup": [1, 5], "matmul248_kernel_config_prun": [1, 5], "gptq_quant_linear": [1, 7], "quantlinear": [1, 5], "forward": [1, 2, 5], "pack": [1, 5], "quantlinearfunct": [1, 5], "backward": [1, 2, 5], "autotune_warmup_linear": [1, 5], "make_quant_linear": [1, 5], "matmul248": [1, 5], "transpose_matmul248": [1, 5], "lmetric_quant_matmul": [1, 7], "get_bnb_weight_st": [1, 5], "lmetric_quant_matmul_lx3hxh": [1, 5], "util": [1, 7], "hf_download": [1, 7], "download_hf_repo": [1, 6], "qkv": 2, "dropout_p": 2, "0": [2, 4, 5], "causal": 2, "true": [2, 5], "sourc": [2, 3, 4, 5, 6], "paramet": [2, 3, 4, 5, 6], "batch_siz": 2, "seqlen": [2, 4, 5], "3": 2, "nhead": 2, "head_dim": 2, "float": [2, 4, 5], "output": [2, 5], "int": [2, 4, 5], "32": 2, "128": [2, 4, 5], "num_head": 2, "64": 2, "96": 2, "160": 2, "dtype": [2, 4, 5], "torch": [2, 3, 4, 5], "float16": [2, 4, 5], "metric": [2, 3, 4, 5], "tflop": [2, 3, 4, 5], "kernel": [2, 3, 4, 5], "triton_flash": 2, "flash2": 2, "xformer": 2, "direct": 2, "fwd": 2, "perform": [2, 3, 5], "benchmark": [2, 5], "differ": [2, 5], "list": [2, 3, 4, 5, 6], "batch": 2, "size": [2, 3, 4, 5], "input": [2, 4, 5], "data": [2, 3, 4, 5], "default": [2, 3, 4, 5, 6], "i": [2, 3, 5], "sequenc": [2, 4, 5], "length": [2, 4, 5], "number": [2, 5], "head": 2, "The": [2, 3, 5], "dimens": [2, 5], "onli": 2, "support": 2, "256": [2, 3], "type": [2, 3, 4, 5, 6], "tensor": [2, 4, 5], "str": [2, 3, 4, 5, 6], "measur": [2, 3], "avail": 2, "option": [2, 3, 4, 5, 6], "ar": [2, 3, 5], "tbp": [2, 3, 4, 5], "m": [2, 3, 4, 5], "function": [2, 5], "us": [2, 3, 5, 6], "comput": [2, 4, 5], "bwd": 2, "fwd_bwd": 2, "return": [2, 3, 4, 5, 6], "none": [2, 3, 5, 6], "896": 3, "1536": 3, "2176": 3, "2816": 3, "3456": 3, "4096": [3, 4, 5], "triton": [3, 5], "matrix": [3, 4, 5], "multipl": [3, 4, 5], "librari": 3, "fp16": [3, 5], "matric": 3, "valid": 3, "we": 3, "test": 3, "b": 3, "activ": 3, "x": [4, 5], "threshold": [4, 5], "coo": 4, "spars": 4, "represent": 4, "base": [4, 5], "given": [4, 5], "valu": [4, 5], "coosparsetensor": 4, "pytorch": 4, "csr": 4, "csc": 4, "bsr": 4, "bsc": 4, "hidden_s": [4, 5], "2048": [4, 5], "bnb_spars": 4, "ratio": 4, "5": 4, "prune": [4, 5], "oper": [4, 5], "hidden": [4, 5], "layer": 4, "torch_spars": 4, "mostli": 5, "same": 5, "few": 5, "chang": 5, "like": 5, "40": 5, "instead": 5, "100": 5, "class": 5, "fn": 5, "arg_nam": 5, "config": 5, "kei": 5, "reset_to_zero": 5, "prune_configs_bi": 5, "dict": 5, "nearest_power_of_two": 5, "bool": [5, 6], "fals": [5, 6], "iter": 5, "debug": 5, "kernelinterfac": 5, "kwarg": 5, "arg": 5, "decor": 5, "auto": 5, "tune": 5, "jit": 5, "d": 5, "highlight": 5, "python": 5, "code": 5, "block": 5, "meta": 5, "block_siz": 5, "num_warp": 5, "4": 5, "1024": 5, "8": 5, "x_size": 5, "two": 5, "abov": 5, "evalu": 5, "anytim": 5, "def": 5, "x_ptr": 5, "note": 5, "when": [5, 6], "all": 5, "configur": 5, "time": 5, "thi": 5, "mean": 5, "whatev": 5, "updat": 5, "To": 5, "avoid": 5, "undesir": 5, "behavior": 5, "you": 5, "can": 5, "argument": 5, "which": 5, "reset": 5, "provid": 5, "zero": 5, "befor": 5, "ani": 5, "object": 5, "name": [5, 6], "whose": 5, "trigger": 5, "field": 5, "perf_model": 5, "model": 5, "predic": 5, "top_k": 5, "bench": 5, "early_config_prun": 5, "do": 5, "earli": 5, "eg": 5, "num_stag": 5, "It": 5, "take": 5, "its": 5, "narg": 5, "main": 5, "purpos": 5, "shrink": 5, "block_size_": 5, "correspond": 5, "smaller": 5, "bit": 5, "groupsiz": 5, "infeatur": 5, "outfeatur": 5, "bia": 5, "defin": 5, "everi": 5, "call": 5, "should": 5, "overridden": 5, "subclass": 5, "although": 5, "recip": 5, "pass": 5, "need": 5, "within": 5, "one": 5, "instanc": 5, "afterward": 5, "sinc": 5, "former": 5, "care": 5, "regist": 5, "hook": 5, "while": 5, "latter": 5, "silent": 5, "ignor": 5, "them": 5, "linear": 5, "scale": 5, "g_idx": 5, "static": 5, "ctx": 5, "grad_output": 5, "formula": 5, "differenti": 5, "mode": 5, "automat": 5, "alia": 5, "vjp": 5, "must": 5, "accept": 5, "context": 5, "first": 5, "follow": 5, "mani": 5, "non": 5, "were": 5, "each": 5, "gradient": 5, "w": 5, "r": 5, "t": 5, "If": 5, "an": 5, "requir": 5, "grad": 5, "just": 5, "retriev": 5, "save": [5, 6], "dure": 5, "also": 5, "ha": 5, "attribut": 5, "needs_input_grad": 5, "tupl": 5, "boolean": 5, "repres": 5, "whether": [5, 6], "e": 5, "g": 5, "have": 5, "qweight": 5, "qzero": 5, "maxq": 5, "There": 5, "wai": 5, "usag": 5, "1": 5, "combin": 5, "staticmethod": 5, "other": 5, "see": 5, "more": 5, "detail": 5, "2": 5, "separ": 5, "setup_context": 5, "longer": 5, "overrid": 5, "autograd": 5, "handl": 5, "set": 5, "up": 5, "extend": 5, "store": 5, "arbitrari": 5, "directli": 5, "though": 5, "current": 5, "enforc": 5, "compat": 5, "either": 5, "save_for_backward": 5, "thei": 5, "intend": 5, "equival": 5, "save_for_forward": 5, "jvp": 5, "transpos": 5, "pre": 5, "quantiz": 5, "weight": 5, "6": 5, "has_fp16_weight": 5, "memory_efficient_backward": 5, "construct": 5, "state": 5, "bnb_weight": 5, "dictionari": 5, "flag": 5, "indic": 5, "format": 5, "memori": 5, "effici": 5, "enabl": 5, "A": 5, "contain": 5, "6144": 5, "8192": 5, "10240": 5, "12288": 5, "14336": 5, "gptq": 5, "bnb_infer": 5, "rang": 5, "group": 5, "repo_id": 6, "repo_typ": 6, "filenam": 6, "white_pattern": 6, "black_pattern": 6, "save_dir": 6, "save_with_symlink": 6, "revis": 6, "download": 6, "repositori": 6, "from": 6, "hug": 6, "face": 6, "hub": 6, "creat": 6, "snapshot": 6, "identifi": 6, "file": 6, "pattern": 6, "includ": 6, "exclud": 6, "directori": 6, "symlink": 6, "local": 6, "rais": 6, "someexcept": 6, "descript": 6, "except": 6, "exampl": 6, "nousresearch": 6, "llama2": 6, "7b": 6, "hf": 6, "subpackag": 7, "content": 7}, "objects": {"": [[1, 0, 0, "-", "lmetric"]], "lmetric": [[2, 0, 0, "-", "attention"], [3, 0, 0, "-", "floating_ops"], [4, 0, 0, "-", "llm_prune"], [5, 0, 0, "-", "llm_quant"], [6, 0, 0, "-", "utils"]], "lmetric.attention": [[2, 0, 0, "-", "lmetric_attention"]], "lmetric.attention.lmetric_attention": [[2, 1, 1, "", "attention_pytorch"], [2, 1, 1, "", "lmetric_attention"]], "lmetric.floating_ops": [[3, 0, 0, "-", "lmetric_matmul"]], "lmetric.floating_ops.lmetric_matmul": [[3, 1, 1, "", "lmetric_matmul_fp16"], [3, 1, 1, "", "matmul"]], "lmetric.llm_prune": [[4, 0, 0, "-", "lmetric_prune_matmul"]], "lmetric.llm_prune.lmetric_prune_matmul": [[4, 1, 1, "", "get_coo"], [4, 1, 1, "", "get_pt_sparse"], [4, 1, 1, "", "lmetric_prune_matmul_Lx3HxH"]], "lmetric.llm_quant": [[5, 0, 0, "-", "custom_autotune"], [5, 0, 0, "-", "gptq_quant_linear"], [5, 0, 0, "-", "lmetric_quant_matmul"]], "lmetric.llm_quant.custom_autotune": [[5, 2, 1, "", "Autotuner"], [5, 1, 1, "", "autotune"], [5, 1, 1, "", "matmul248_kernel_config_pruner"]], "lmetric.llm_quant.custom_autotune.Autotuner": [[5, 3, 1, "", "prune_configs"], [5, 3, 1, "", "run"], [5, 3, 1, "", "warmup"]], "lmetric.llm_quant.gptq_quant_linear": [[5, 2, 1, "", "QuantLinear"], [5, 2, 1, "", "QuantLinearFunction"], [5, 1, 1, "", "autotune_warmup_linear"], [5, 1, 1, "", "make_quant_linear"], [5, 1, 1, "", "matmul248"], [5, 1, 1, "", "transpose_matmul248"]], "lmetric.llm_quant.gptq_quant_linear.QuantLinear": [[5, 3, 1, "", "forward"], [5, 3, 1, "", "pack"]], "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction": [[5, 3, 1, "", "backward"], [5, 3, 1, "", "forward"]], "lmetric.llm_quant.lmetric_quant_matmul": [[5, 1, 1, "", "get_bnb_weight_state"], [5, 1, 1, "", "lmetric_quant_matmul_Lx3HxH"]], "lmetric.utils": [[6, 0, 0, "-", "hf_download"]], "lmetric.utils.hf_download": [[6, 1, 1, "", "download_hf_repo"]]}, "objtypes": {"0": "py:module", "1": "py:function", "2": "py:class", "3": "py:method"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "function", "Python function"], "2": ["py", "class", "Python class"], "3": ["py", "method", "Python method"]}, "titleterms": {"welcom": 0, "lmetric": [0, 1, 2, 3, 4, 5, 6, 7], "": 0, "document": 0, "content": [0, 1, 2, 3, 4, 5, 6], "indic": 0, "tabl": 0, "packag": [1, 2, 3, 4, 5, 6], "subpackag": 1, "modul": [1, 2, 3, 4, 5, 6], "attent": 2, "submodul": [2, 3, 4, 5, 6], "lmetric_attent": 2, "floating_op": 3, "lmetric_matmul": 3, "llm_prune": 4, "lmetric_prune_matmul": 4, "llm_quant": 5, "custom_autotun": 5, "gptq_quant_linear": 5, "lmetric_quant_matmul": 5, "util": 6, "hf_download": 6}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.viewcode": 1, "sphinx": 57}, "alltitles": {"Welcome to lmetric\u2019s documentation!": [[0, "welcome-to-lmetric-s-documentation"]], "Contents:": [[0, null]], "Indices and tables": [[0, "indices-and-tables"]], "lmetric package": [[1, "lmetric-package"]], "Subpackages": [[1, "subpackages"]], "Module contents": [[1, "module-lmetric"], [2, "module-lmetric.attention"], [3, "module-lmetric.floating_ops"], [4, "module-lmetric.llm_prune"], [5, "module-lmetric.llm_quant"], [6, "module-lmetric.utils"]], "lmetric.attention package": [[2, "lmetric-attention-package"]], "Submodules": [[2, "submodules"], [3, "submodules"], [4, "submodules"], [5, "submodules"], [6, "submodules"]], "lmetric.attention.lmetric_attention module": [[2, "module-lmetric.attention.lmetric_attention"]], "lmetric.floating_ops package": [[3, "lmetric-floating-ops-package"]], "lmetric.floating_ops.lmetric_matmul module": [[3, "module-lmetric.floating_ops.lmetric_matmul"]], "lmetric.llm_prune package": [[4, "lmetric-llm-prune-package"]], "lmetric.llm_prune.lmetric_prune_matmul module": [[4, "module-lmetric.llm_prune.lmetric_prune_matmul"]], "lmetric.llm_quant package": [[5, "lmetric-llm-quant-package"]], "lmetric.llm_quant.custom_autotune module": [[5, "module-lmetric.llm_quant.custom_autotune"]], "lmetric.llm_quant.gptq_quant_linear module": [[5, "module-lmetric.llm_quant.gptq_quant_linear"]], "lmetric.llm_quant.lmetric_quant_matmul module": [[5, "module-lmetric.llm_quant.lmetric_quant_matmul"]], "lmetric.utils package": [[6, "lmetric-utils-package"]], "lmetric.utils.hf_download module": [[6, "module-lmetric.utils.hf_download"]], "lmetric": [[7, "lmetric"]]}, "indexentries": {"lmetric": [[1, "module-lmetric"]], "module": [[1, "module-lmetric"], [2, "module-lmetric.attention"], [2, "module-lmetric.attention.lmetric_attention"], [3, "module-lmetric.floating_ops"], [3, "module-lmetric.floating_ops.lmetric_matmul"], [4, "module-lmetric.llm_prune"], [4, "module-lmetric.llm_prune.lmetric_prune_matmul"], [5, "module-lmetric.llm_quant"], [5, "module-lmetric.llm_quant.custom_autotune"], [5, "module-lmetric.llm_quant.gptq_quant_linear"], [5, "module-lmetric.llm_quant.lmetric_quant_matmul"], [6, "module-lmetric.utils"], [6, "module-lmetric.utils.hf_download"]], "attention_pytorch() (in module lmetric.attention.lmetric_attention)": [[2, "lmetric.attention.lmetric_attention.attention_pytorch"]], "lmetric.attention": [[2, "module-lmetric.attention"]], "lmetric.attention.lmetric_attention": [[2, "module-lmetric.attention.lmetric_attention"]], "lmetric_attention() (in module lmetric.attention.lmetric_attention)": [[2, "lmetric.attention.lmetric_attention.lmetric_attention"]], "lmetric.floating_ops": [[3, "module-lmetric.floating_ops"]], "lmetric.floating_ops.lmetric_matmul": [[3, "module-lmetric.floating_ops.lmetric_matmul"]], "lmetric_matmul_fp16() (in module lmetric.floating_ops.lmetric_matmul)": [[3, "lmetric.floating_ops.lmetric_matmul.lmetric_matmul_fp16"]], "matmul() (in module lmetric.floating_ops.lmetric_matmul)": [[3, "lmetric.floating_ops.lmetric_matmul.matmul"]], "get_coo() (in module lmetric.llm_prune.lmetric_prune_matmul)": [[4, "lmetric.llm_prune.lmetric_prune_matmul.get_coo"]], "get_pt_sparse() (in module lmetric.llm_prune.lmetric_prune_matmul)": [[4, "lmetric.llm_prune.lmetric_prune_matmul.get_pt_sparse"]], "lmetric.llm_prune": [[4, "module-lmetric.llm_prune"]], "lmetric.llm_prune.lmetric_prune_matmul": [[4, "module-lmetric.llm_prune.lmetric_prune_matmul"]], "lmetric_prune_matmul_lx3hxh() (in module lmetric.llm_prune.lmetric_prune_matmul)": [[4, "lmetric.llm_prune.lmetric_prune_matmul.lmetric_prune_matmul_Lx3HxH"]], "autotuner (class in lmetric.llm_quant.custom_autotune)": [[5, "lmetric.llm_quant.custom_autotune.Autotuner"]], "quantlinear (class in lmetric.llm_quant.gptq_quant_linear)": [[5, "lmetric.llm_quant.gptq_quant_linear.QuantLinear"]], "quantlinearfunction (class in lmetric.llm_quant.gptq_quant_linear)": [[5, "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction"]], "autotune() (in module lmetric.llm_quant.custom_autotune)": [[5, "lmetric.llm_quant.custom_autotune.autotune"]], "autotune_warmup_linear() (in module lmetric.llm_quant.gptq_quant_linear)": [[5, "lmetric.llm_quant.gptq_quant_linear.autotune_warmup_linear"]], "backward() (lmetric.llm_quant.gptq_quant_linear.quantlinearfunction static method)": [[5, "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.backward"]], "forward() (lmetric.llm_quant.gptq_quant_linear.quantlinear method)": [[5, "lmetric.llm_quant.gptq_quant_linear.QuantLinear.forward"]], "forward() (lmetric.llm_quant.gptq_quant_linear.quantlinearfunction static method)": [[5, "lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward"]], "get_bnb_weight_state() (in module lmetric.llm_quant.lmetric_quant_matmul)": [[5, "lmetric.llm_quant.lmetric_quant_matmul.get_bnb_weight_state"]], "lmetric.llm_quant": [[5, "module-lmetric.llm_quant"]], "lmetric.llm_quant.custom_autotune": [[5, "module-lmetric.llm_quant.custom_autotune"]], "lmetric.llm_quant.gptq_quant_linear": [[5, "module-lmetric.llm_quant.gptq_quant_linear"]], "lmetric.llm_quant.lmetric_quant_matmul": [[5, "module-lmetric.llm_quant.lmetric_quant_matmul"]], "lmetric_quant_matmul_lx3hxh() (in module lmetric.llm_quant.lmetric_quant_matmul)": [[5, "lmetric.llm_quant.lmetric_quant_matmul.lmetric_quant_matmul_Lx3HxH"]], "make_quant_linear() (in module lmetric.llm_quant.gptq_quant_linear)": [[5, "lmetric.llm_quant.gptq_quant_linear.make_quant_linear"]], "matmul248() (in module lmetric.llm_quant.gptq_quant_linear)": [[5, "lmetric.llm_quant.gptq_quant_linear.matmul248"]], "matmul248_kernel_config_pruner() (in module lmetric.llm_quant.custom_autotune)": [[5, "lmetric.llm_quant.custom_autotune.matmul248_kernel_config_pruner"]], "pack() (lmetric.llm_quant.gptq_quant_linear.quantlinear method)": [[5, "lmetric.llm_quant.gptq_quant_linear.QuantLinear.pack"]], "prune_configs() (lmetric.llm_quant.custom_autotune.autotuner method)": [[5, "lmetric.llm_quant.custom_autotune.Autotuner.prune_configs"]], "run() (lmetric.llm_quant.custom_autotune.autotuner method)": [[5, "lmetric.llm_quant.custom_autotune.Autotuner.run"]], "transpose_matmul248() (in module lmetric.llm_quant.gptq_quant_linear)": [[5, "lmetric.llm_quant.gptq_quant_linear.transpose_matmul248"]], "warmup() (lmetric.llm_quant.custom_autotune.autotuner method)": [[5, "lmetric.llm_quant.custom_autotune.Autotuner.warmup"]], "download_hf_repo() (in module lmetric.utils.hf_download)": [[6, "lmetric.utils.hf_download.download_hf_repo"]], "lmetric.utils": [[6, "module-lmetric.utils"]], "lmetric.utils.hf_download": [[6, "module-lmetric.utils.hf_download"]]}})