<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>lmetric.llm_quant package &mdash; lmetric 0.0.2 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="lmetric.utils package" href="lmetric.utils.html" />
    <link rel="prev" title="lmetric.llm_prune package" href="lmetric.llm_prune.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            lmetric
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">lmetric</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="lmetric.html">lmetric package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="lmetric.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="lmetric.floating_ops.html">lmetric.floating_ops package</a></li>
<li class="toctree-l4"><a class="reference internal" href="lmetric.llm_prune.html">lmetric.llm_prune package</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">lmetric.llm_quant package</a></li>
<li class="toctree-l4"><a class="reference internal" href="lmetric.utils.html">lmetric.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="lmetric.html#module-lmetric">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">lmetric</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">lmetric</a></li>
          <li class="breadcrumb-item"><a href="lmetric.html">lmetric package</a></li>
      <li class="breadcrumb-item active">lmetric.llm_quant package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lmetric.llm_quant.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lmetric-llm-quant-package">
<h1>lmetric.llm_quant package<a class="headerlink" href="#lmetric-llm-quant-package" title="Permalink to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this heading"></a></h2>
</section>
<section id="module-lmetric.llm_quant.custom_autotune">
<span id="lmetric-llm-quant-custom-autotune-module"></span><h2>lmetric.llm_quant.custom_autotune module<a class="headerlink" href="#module-lmetric.llm_quant.custom_autotune" title="Permalink to this heading"></a></h2>
<p>Mostly the same as the autotuner in Triton, but with a few changes like using 40 runs instead of 100.</p>
<dl class="py class">
<dt class="sig sig-object py" id="lmetric.llm_quant.custom_autotune.Autotuner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.custom_autotune.</span></span><span class="sig-name descname"><span class="pre">Autotuner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">arg_names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">configs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_to_zero</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prune_configs_by</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nearest_power_of_two</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/custom_autotune.html#Autotuner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.custom_autotune.Autotuner" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">KernelInterface</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="lmetric.llm_quant.custom_autotune.Autotuner.prune_configs">
<span class="sig-name descname"><span class="pre">prune_configs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/custom_autotune.html#Autotuner.prune_configs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.custom_autotune.Autotuner.prune_configs" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lmetric.llm_quant.custom_autotune.Autotuner.run">
<span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/custom_autotune.html#Autotuner.run"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.custom_autotune.Autotuner.run" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lmetric.llm_quant.custom_autotune.Autotuner.warmup">
<span class="sig-name descname"><span class="pre">warmup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/custom_autotune.html#Autotuner.warmup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.custom_autotune.Autotuner.warmup" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.custom_autotune.autotune">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.custom_autotune.</span></span><span class="sig-name descname"><span class="pre">autotune</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">configs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prune_configs_by</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reset_to_zero</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nearest_power_of_two</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">debug</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/custom_autotune.html#autotune"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.custom_autotune.autotune" title="Permalink to this definition"></a></dt>
<dd><p>Decorator for auto-tuning a <code class="code docutils literal notranslate"><span class="pre">triton.jit</span></code>’d function.
.. highlight:: python
.. code-block:: python</p>
<blockquote>
<div><dl>
<dt>&#64;triton.autotune(configs=[</dt><dd><p>triton.Config(meta={‘BLOCK_SIZE’: 128}, num_warps=4),
triton.Config(meta={‘BLOCK_SIZE’: 1024}, num_warps=8),
],
key=[‘x_size’] # the two above configs will be evaluated anytime</p>
<blockquote>
<div><p># the value of x_size changes</p>
</div></blockquote>
</dd>
</dl>
<p>)
&#64;triton.jit
def kernel(x_ptr, x_size, <a href="#id1"><span class="problematic" id="id2">**</span></a>META):</p>
<blockquote>
<div><p>BLOCK_SIZE = META[‘BLOCK_SIZE’]</p>
</div></blockquote>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Note<span class="colon">:</span></dt>
<dd class="field-odd"><p>When all the configurations are evaluated, the kernel will run multiple time.
This means that whatever value the kernel updates will be updated multiple times.
To avoid this undesired behavior, you can use the <cite>reset_to_zero</cite> argument, which
reset the value of the provided tensor to <cite>zero</cite> before running any configuration.</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>configs</strong> (<em>list</em><em>[</em><em>triton.Config</em><em>]</em>) – a list of <code class="code docutils literal notranslate"><span class="pre">triton.Config</span></code> objects</p></li>
<li><p><strong>key</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) – a list of argument names whose change in value will trigger the evaluation of all provided configs.</p></li>
<li><p><strong>prune_configs_by</strong> – a dict of functions that are used to prune configs, fields:
‘perf_model’: performance model used to predicate running time with different configs, returns running time
‘top_k’: number of configs to bench
‘early_config_prune’(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.</p></li>
<li><p><strong>reset_to_zero</strong> (<em>list</em><em>[</em><em>str</em><em>]</em>) – a list of argument names whose value will be reset to zero before evaluating any configs.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.custom_autotune.matmul248_kernel_config_pruner">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.custom_autotune.</span></span><span class="sig-name descname"><span class="pre">matmul248_kernel_config_pruner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">configs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/custom_autotune.html#matmul248_kernel_config_pruner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.custom_autotune.matmul248_kernel_config_pruner" title="Permalink to this definition"></a></dt>
<dd><p>The main purpose of this function is to shrink BLOCK_SIZE_* when the corresponding dimension is smaller.</p>
</dd></dl>

</section>
<section id="module-lmetric.llm_quant.gptq_quant_linear">
<span id="lmetric-llm-quant-gptq-quant-linear-module"></span><h2>lmetric.llm_quant.gptq_quant_linear module<a class="headerlink" href="#module-lmetric.llm_quant.gptq_quant_linear" title="Permalink to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.QuantLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.gptq_quant_linear.</span></span><span class="sig-name descname"><span class="pre">QuantLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">bits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groupsize</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">infeatures</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outfeatures</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#QuantLinear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinear" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.QuantLinear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#QuantLinear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinear.forward" title="Permalink to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.QuantLinear.pack">
<span class="sig-name descname"><span class="pre">pack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">linear</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scales</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zeros</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g_idx</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#QuantLinear.pack"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinear.pack" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.gptq_quant_linear.</span></span><span class="sig-name descname"><span class="pre">QuantLinearFunction</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#QuantLinearFunction"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_output</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#QuantLinearFunction.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.backward" title="Permalink to this definition"></a></dt>
<dd><p>Defines a formula for differentiating the operation with backward mode
automatic differentiation (alias to the vjp function).</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward" title="lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward" title="lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.backward" title="lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward()</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward" title="lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> needs gradient computated w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qweight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scales</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qzeros</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxq</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#QuantLinearFunction.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.QuantLinearFunction.forward" title="Permalink to this definition"></a></dt>
<dd><p>This function is to be overridden by all subclasses. There are two ways
to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <span class="xref std std-ref">combining-forward-context</span> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">output</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context()</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <span class="xref std std-ref">extending-autograd</span> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward()</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward()</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.autotune_warmup_linear">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.gptq_quant_linear.</span></span><span class="sig-name descname"><span class="pre">autotune_warmup_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transpose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#autotune_warmup_linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.autotune_warmup_linear" title="Permalink to this definition"></a></dt>
<dd><p>Pre-tunes the quantized kernel</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.make_quant_linear">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.gptq_quant_linear.</span></span><span class="sig-name descname"><span class="pre">make_quant_linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">names</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groupsize</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#make_quant_linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.make_quant_linear" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.matmul248">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.gptq_quant_linear.</span></span><span class="sig-name descname"><span class="pre">matmul248</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qweight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scales</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qzeros</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxq</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#matmul248"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.matmul248" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.gptq_quant_linear.transpose_matmul248">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.gptq_quant_linear.</span></span><span class="sig-name descname"><span class="pre">transpose_matmul248</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qweight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scales</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qzeros</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">g_idx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxq</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/gptq_quant_linear.html#transpose_matmul248"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.gptq_quant_linear.transpose_matmul248" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="module-lmetric.llm_quant.lmetric_quant_matmul">
<span id="lmetric-llm-quant-lmetric-quant-matmul-module"></span><h2>lmetric.llm_quant.lmetric_quant_matmul module<a class="headerlink" href="#module-lmetric.llm_quant.lmetric_quant_matmul" title="Permalink to this heading"></a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.lmetric_quant_matmul.get_bnb_weight_state">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.lmetric_quant_matmul.</span></span><span class="sig-name descname"><span class="pre">get_bnb_weight_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">weight</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">6.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">has_fp16_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_efficient_backward</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/lmetric_quant_matmul.html#get_bnb_weight_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.lmetric_quant_matmul.get_bnb_weight_state" title="Permalink to this definition"></a></dt>
<dd><p>Constructs a state and a bnb_weight dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>weight</strong> (<em>Tensor</em>) – The weight tensor.</p></li>
<li><p><strong>threshold</strong> (<em>float</em><em>, </em><em>optional</em>) – Threshold value. Defaults to 6.0.</p></li>
<li><p><strong>has_fp16_weights</strong> (<em>bool</em><em>, </em><em>optional</em>) – Flag indicating if the weights are in fp16 format. Defaults to False.</p></li>
<li><p><strong>memory_efficient_backward</strong> (<em>bool</em><em>, </em><em>optional</em>) – Flag indicating if memory-efficient backward is enabled. Defaults to False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A dictionary containing the ‘bnb_weight’ and ‘state’.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="lmetric.llm_quant.lmetric_quant_matmul.lmetric_quant_matmul_Lx3HxH">
<span class="sig-prename descclassname"><span class="pre">lmetric.llm_quant.lmetric_quant_matmul.</span></span><span class="sig-name descname"><span class="pre">lmetric_quant_matmul_Lx3HxH</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">seqlen</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[2048,</span> <span class="pre">4096,</span> <span class="pre">6144,</span> <span class="pre">8192,</span> <span class="pre">10240,</span> <span class="pre">12288,</span> <span class="pre">14336]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.float16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'TFLOPS'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['torch',</span> <span class="pre">'gptq',</span> <span class="pre">'bnb_infer']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">groupsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/lmetric/llm_quant/lmetric_quant_matmul.html#lmetric_quant_matmul_Lx3HxH"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#lmetric.llm_quant.lmetric_quant_matmul.lmetric_quant_matmul_Lx3HxH" title="Permalink to this definition"></a></dt>
<dd><p>Performs a quantized matrix multiplication benchmarking with different kernels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>seqlen</strong> (<em>int</em><em>, </em><em>optional</em>) – Sequence length. Defaults to 128.</p></li>
<li><p><strong>hidden_sizes</strong> (<em>list</em><em>, </em><em>optional</em>) – List of hidden sizes. Defaults to [2048 * i for i in range(1, 8)].</p></li>
<li><p><strong>dtype</strong> (<em>torch.dtype</em><em>, </em><em>optional</em>) – Data type. Defaults to torch.float16.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em>, </em><em>optional</em>) – Metric for benchmarking. Defaults to ‘TFLOPS’. options: TFLOPS, TBPS, MS</p></li>
<li><p><strong>kernels</strong> (<em>list</em><em>, </em><em>optional</em>) – List of kernels to use. Defaults to [‘torch’, ‘gptq’, ‘bnb_infer’].</p></li>
<li><p><strong>bits</strong> (<em>int</em><em>, </em><em>optional</em>) – Number of bits for quantization. Defaults to 4.</p></li>
<li><p><strong>groupsize</strong> (<em>int</em><em>, </em><em>optional</em>) – Group size for quantization. Defaults to 128.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-lmetric.llm_quant">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-lmetric.llm_quant" title="Permalink to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lmetric.llm_prune.html" class="btn btn-neutral float-left" title="lmetric.llm_prune package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="lmetric.utils.html" class="btn btn-neutral float-right" title="lmetric.utils package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, zhangzn710@gmail.com.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>